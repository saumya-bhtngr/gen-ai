{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Objective\n",
        "LLama Index is a framework for that seamlessly handles steps in a RAG pipeline: chunking, embedding, indexing, creating vector store, retrieval from vector store.\n",
        "This document demonstrates use cases of llama index using sample text documents from insurance domain. Following examples are included:\n",
        "\n",
        "\n",
        "*   Q&A over a single document\n",
        "*   Multiple Q&A over multiple documents\n",
        "*   Storing (persisting) llama index locally on a disk so that everytime program runs, we may not have to re-index the documents\n",
        "*   Custom settings for chunk size and model used\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E0B5qZetZHf_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oBCVesunY5H5",
        "outputId": "1c5cf29d-7e02-43aa-eeb5-e3cebaec94b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-embeddings-huggingface\n",
            "  Downloading llama_index_embeddings_huggingface-0.6.1-py3-none-any.whl.metadata (458 bytes)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.1)\n",
            "Requirement already satisfied: llama-index-core<0.15,>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-huggingface) (0.14.15)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-huggingface) (5.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.24.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.3)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.67.3)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.24.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.15.0)\n",
            "\u001b[33mWARNING: huggingface-hub 1.4.1 does not provide the extra 'inference'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.13.3)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.22.1)\n",
            "Requirement already satisfied: banks<3,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.4.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.2.0)\n",
            "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.15.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.6.1)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (4.9.2)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.12.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (82.0.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.0.47)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (9.1.4)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.12.0)\n",
            "Requirement already satisfied: tinytag>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.2.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.17.3)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (5.0.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.10.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.16.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.22.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.3.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.3.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.1.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2025.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.3.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.14.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.7.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.26.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.6.0)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.24.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.0.4)\n",
            "Requirement already satisfied: griffecli==2.0.0 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.3.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.0.0)\n",
            "Requirement already satisfied: griffelib==2.0.0 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.3.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.0.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffecli==2.0.0->griffe->banks<3,>=2.3.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.3.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.1.2)\n",
            "Downloading llama_index_embeddings_huggingface-0.6.1-py3-none-any.whl (8.9 kB)\n",
            "Installing collected packages: llama-index-embeddings-huggingface\n",
            "Successfully installed llama-index-embeddings-huggingface-0.6.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "llama_index"
                ]
              },
              "id": "9cbfe72dc8bd48d8a1465578d0b49a1b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#!pip install llama-index\n",
        "!pip install llama-index-embeddings-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from llama_index.core import Document, VectorStoreIndex, StorageContext, load_index_from_storage"
      ],
      "metadata": {
        "id": "XUEE9-IbaQkJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ],
      "metadata": {
        "id": "1Z2pTIOShKSq"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question-Answering Over a Single Document"
      ],
      "metadata": {
        "id": "O4QpH_kniINg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_use_case():\n",
        "  \"\"\" This function demonstrates basic use case for llama index. It automates:\n",
        "  Step 1: Convert raw text to Document type for llama-index\n",
        "  Step 2: Create a VectorStoreIndex that handles chunking, embeddings using OpenAIEmbeddings, stores embeddings in an in-memory vector store\n",
        "  Step 3: Create a query engine. Query engine handles the following: embded user query -> finds similar chunks -> pass to LLM -> generate response\n",
        "  Step 4: User asks questions\n",
        "  \"\"\"\n",
        "\n",
        "  # sample text for document (could be a .pdf or any other text file)\n",
        "  raw_text = \"\"\"\n",
        "  Policy Number: CYB-2026-1.\n",
        "  This cyber liability policy covers data breaches, ransomware attacks,\n",
        "  and business interruption losses up to $5 million per occurrence.\n",
        "  The retroactive date is February 29, 2026.\n",
        "  Exclusions include: war, nuclear events, and intentional acts by the insured.\n",
        "  Annual premium: $80,000. Renewal date: December 31, 2028.\n",
        "  \"\"\"\n",
        "\n",
        "  document = Document(text=raw_text, metadata={\"source_document\":\"CYB-2026-1\"})\n",
        "\n",
        "  # build the index\n",
        "  # VectorStoreIndex implicitly uses OpenAI's OpenAI Embedding model to generate responses\n",
        "  # once we've set the openai API key above.\n",
        "\n",
        "  index = VectorStoreIndex.from_documents([document])\n",
        "\n",
        "  # questions are asked to the query engine\n",
        "  query_engine = index.as_query_engine()\n",
        "\n",
        "  user_query = \"What are the inclusions and exclusions in policy document\"\n",
        "\n",
        "  #.query() implicitly uses OpenAI's LLM model to generate responses once we've set the openai API key above.\n",
        "  response = query_engine.query(user_query)\n",
        "  print(response)\n",
        "\n",
        "  # print source of the response; a node represents a chunk of the document\n",
        "  for i, node in enumerate(response.source_nodes):\n",
        "    print(f\"Chunk used: {i+1}\")\n",
        "    print(f\"Score: {node.score}\")\n",
        "    print(f\"Source Document: {node.text[:150]}\")\n"
      ],
      "metadata": {
        "id": "RkZZB4uxZH-A"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question-Answering over Multiple Documents"
      ],
      "metadata": {
        "id": "svVXHFSUiFDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multiple_doc_use_case():\n",
        "  # list of three separate documents\n",
        "  document = [\n",
        "      Document(\n",
        "          text=\"\"\"Policy Number: CYB-2024-10234.\n",
        "            Cyber liability policy covering data breaches and ransomware.\n",
        "            Coverage limit: $5 million per occurrence.\n",
        "            Exclusions: war, nuclear events, intentional acts.\"\"\",\n",
        "          metadata={\"doc_type\": \"cyber_policy\", \"client\": \"ABC Corp\"}\n",
        "          ),\n",
        "      Document(\n",
        "          text=\"\"\"Policy Number: GL-2024-88821.\n",
        "            General Liability policy for bodily injury and property damage.\n",
        "            Coverage limit: $1 million per occurrence, $2 million aggregate.\n",
        "            Covers operations in Singapore, Malaysia, and Indonesia.\"\"\",\n",
        "          metadata={\"doc_type\": \"general_liability\", \"client\": \"ABC Corp\"}\n",
        "          ),\n",
        "      Document(\n",
        "          text=\"\"\"Claims History for ABC Corp:\n",
        "            2022: One cyber claim for $120,000. Phishing attack, resolved.\n",
        "            2023: One GL claim for $45,000. Slip and fall, settled.\n",
        "            2024: No claims filed. 3-year loss ratio: 0.68.\"\"\",\n",
        "          metadata={\"doc_type\": \"claims_history\", \"client\": \"ABC Corp\"}\n",
        "          )\n",
        "  ]\n",
        "\n",
        "  # build a common index for all documents\n",
        "  index = VectorStoreIndex.from_documents(document)\n",
        "\n",
        "  # retrive top 2 most relevant chunks\n",
        "  query_engine = index.as_query_engine(similarity_top_k=2)\n",
        "\n",
        "  questions = [\"What is the cyber coverage limit?\",\n",
        "              \"Which countries does the GL policy cover?\",\n",
        "              \"Has ABC Corp filed any cyber claims before?\"\n",
        "               ]\n",
        "  for question in questions:\n",
        "    response = query_engine.query(question)\n",
        "    print(\"Question:\\n\", question)\n",
        "    print(\"Response:\\n\", response)"
      ],
      "metadata": {
        "id": "ptGk-V_PiFZL"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving the Index to Disk\n",
        "By default the Llama index is stored in memory, hence, everytime we run the program, it indexes the document(s) again.\n",
        "To avoid re-indexing the files again and again, the index can be stored persistently on disk."
      ],
      "metadata": {
        "id": "vbkQW9tL0rXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# directory to save index\n",
        "PERSIST_DIRECTORY = \"saved_index\""
      ],
      "metadata": {
        "id": "uILvFat0vUWw"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_save_index():\n",
        "  raw_text = \"\"\"\n",
        "   Policy Number: CYB-2024-10234.\n",
        "   Cyber liability policy. Coverage: $5M. Exclusions: war, nuclear.\n",
        "  \"\"\"\n",
        "\n",
        "  document = Document(text=raw_text)\n",
        "  index = VectorStoreIndex.from_documents([document])\n",
        "  # save the index\n",
        "  # https://developers.llamaindex.ai/python/framework/module_guides/storing/save_load/\n",
        "  index.storage_context.persist(persist_dir=PERSIST_DIRECTORY)\n",
        "  print(\"Index saved to: \", PERSIST_DIRECTORY)\n",
        "  return index"
      ],
      "metadata": {
        "id": "It0iay4S0p4E"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_index():\n",
        "  # check if the stored index already exists\n",
        "  if not os.path.exists(PERSIST_DIRECTORY):\n",
        "    print(\"Building index from scratch\")\n",
        "    index=create_and_save_index()\n",
        "  else:\n",
        "    # load the index\n",
        "    print(\"Loading already saved index from: \", PERSIST_DIRECTORY)\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIRECTORY)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "    print(\"Index loaded\")\n",
        "  return index"
      ],
      "metadata": {
        "id": "K3eQSjf31r26"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def persistance_use_case():\n",
        "  index = load_index()\n",
        "  query_engine = index.as_query_engine()\n",
        "  user_question = \"What is the coverage limit?\"\n",
        "  response = query_engine.query(user_question)\n",
        "  print(\"User question: \", user_question)\n",
        "  print(\"Response: \", response)"
      ],
      "metadata": {
        "id": "NfcY-OE-3--A"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  #basic_use_case()\n",
        "  #multiple_doc_use_case()\n",
        "  persistance_use_case()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n93oKm-4kTj",
        "outputId": "d29c61bf-8e02-4e93-fe83-3c8e850b8cd0"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building index from scratch\n",
            "Index saved to:  saved_index\n",
            "User question:  What is the coverage limit?\n",
            "Response:  The coverage limit is $5 million.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5a9Ctrlr4peC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}